# Machine Learning 

by Andrew Ng

## Week 1

### Introduction

ç½‘é¡µæœç´¢â€¦è‡ªåŠ¨é©¾é©¶â€¦AIçš„æœªæ¥æ˜¯æ¨¡æ‹Ÿäººè„‘çš„è¿ä½œæ–¹å¼(ç¥ç»ç½‘ç»œ)ã€‚

ä¸¤ä¸ªå®šä¹‰ï¼š

- Arthur Samuel (1959) ï¼šæœºå™¨å­¦ä¹ æ˜¯gives computers the ability to learn, without being explicitly programmed.

- Tom Mitchell (1998) : "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E." ç»éªŒEï¼Œè¯„ä»·å‡½æ•°Pï¼Œäº‹ä»¶Tã€‚

é€šå¸¸æ¥è¯´ï¼Œæ‰€æœ‰çš„æœºå™¨å­¦ä¹ é—®é¢˜éƒ½å¯ä»¥å½’å…¥ä»¥ä¸‹ä¸¤ç±»ï¼šç›‘ç£å­¦ä¹ å’Œéç›‘ç£å­¦ä¹ ã€‚Supervised learning and Unsupervised learning.

#### Supervised Learning

ç»™äº†ä¸€ä¸ª**æ•°æ®é›†**ä½œä¸º**æ­£ç¡®ç­”æ¡ˆ**ï¼Œæˆ‘ä»¬çŸ¥é“æˆ‘ä»¬è¦ä»€ä¹ˆï¼š

- Regression: é¢„æµ‹è¿ç»­çš„æ•°å€¼ï¼ˆæˆ¿å±‹ä»·æ ¼ï¼‰

- Classification: é¢„æµ‹ä¸è¿ç»­çš„æ•°å€¼ï¼ˆæ¶æ€§è‚¿ç˜¤ï¼Ÿï¼‰

*æœ‰æ›´å¤šçš„å‚æ•°æ—¶å°±å˜æˆé«˜ç»´äº†ï¼Ÿ*

#### Unsupervised Learning

ç»™äº†ä¸€ä¸ª**æ•°æ®é›†**ï¼Œæˆ‘ä»¬ä¸çŸ¥é“æˆ‘ä»¬è¦ä»€ä¹ˆï¼Œèƒ½æ‰¾åˆ°æŸç§ç»“æ„å—ï¼Ÿ

- Clustering: ç»™åŸºå› åˆ†ç»„
- Non-Clustering: æŠŠä¸¤ä¸ªå£°æºçš„å£°éŸ³åˆ†å¼€

### Model and Cost Function

$x(i)$ to denote the â€œ**input**â€ variables (living area in this example), also called input features, and $y^{(i)}$ to denote the â€œ**output**â€ or target variable that we are trying to predict (price). A pair $(x^{(i)} , y^{(i)} )$ is called a **training example**, and the dataset that weâ€™ll be using to learnâ€”a list of $m$ training examples$ {(x^{(i)} , y^{(i)} ); i = 1, . . . , m}$â€”is called a **training set**.

Our goal is, given a training set, to learn a function $h : X â†’ Y$ so that $h(x)$ is a â€œgoodâ€ predictor for the corresponding value of $y$. For historical reasons, this function h is called a **hypothesis**.

**Cost Function**: (Loss) é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®è·ã€‚é™¤ä»¥2æ˜¯ä¸ºäº†æ¢¯åº¦ä¸‹é™æ—¶å’Œå¾®åˆ†çš„2çº¦æ‰ã€‚
$$
J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(\hat{y}_{i}-y_{i}\right)^{2}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x_{i}\right)-y_{i}\right)^{2}
$$
### Parameter Learning

**Gradient descent**: 

> $:=$è¡¨ç¤ºèµ‹å€¼

åå¤æ›´æ–°$\theta$ç›´è‡³æ”¶æ•›ã€‚æ„ä¹‰æ˜¯æ²¿ç€æ–œå¡èµ°äº†ä¸€å°æ­¥ä¸‹å±±ã€‚
$$
\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right)
$$
$$
\begin{aligned}
\frac{\partial}{\partial \theta_{j}} J(\theta) &=\frac{\partial}{\partial \theta_{j}} \frac{1}{2}\left(h_{\theta}(x)-y\right)^{2} \\
&=2 \cdot \frac{1}{2}\left(h_{\theta}(x)-y\right) \cdot \frac{\partial}{\partial \theta_{j}}\left(h_{\theta}(x)-y\right) \\
&=\left(h_{\theta}(x)-y\right) \cdot \frac{\partial}{\partial \theta_{j}}\left(\sum_{i=0}^{n} \theta_{i} x_{i}-y\right) \\
&=\left(h_{\theta}(x)-y\right) x_{j}
\end{aligned}
$$

$$
\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
$$

> æ±‚å’Œéƒ¨åˆ†æ˜¯çŸ©é˜µä¹˜æ³•ä¹˜å‡ºæ¥çš„æ ‡é‡

$\alpha$ is the **learning rate** (æ­¥é•¿). å¤ªå°äº†æ”¶æ•›æ…¢ï¼Œå¤ªå¤§äº†å¯èƒ½ä¼šéœ‡è¡ã€‚

æ¢¯åº¦ä¸‹é™æ³•æ‰¾åˆ°çš„æ˜¯å±€éƒ¨æå°å€¼ã€‚ä½†æ˜¯æ˜¯å‡¸å‡½æ•°ï¼Œæ‰€ä»¥æå°å€¼å°±æ˜¯å…¨å±€æœ€å°å€¼ã€‚å› ä¸º$J$æ˜¯æ ‡å‡†å·®ï¼Œæ˜¯å‡¸å‡½æ•°(convex quadratic function)ã€‚

**Batch**: Each step of gradient descent uses all the training examples.

## Week 2

### Multiple features (multivariate linear regression)

$x_i$ è¡¨ç¤ºç¬¬$i$ç§æ•°æ®ç±»å‹ï¼Œ$x^{(j)}$ è¡¨ç¤ºç¬¬$j$ç»„æ•°æ®ã€‚ä¸ºäº†è®°å·çš„æ–¹ä¾¿ï¼Œå®šä¹‰$x_0^{(i)}=1$ã€‚
$$
\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{j}^{(i)} \quad \text { for } \mathrm{j}:=0 \ldots \mathrm{n}
$$

### Gradient descent in PRACTICE (Tips)

- Feature Scaling & Mean Normalization

  é€šè¿‡æ§åˆ¶å‚æ•°èŒƒå›´åœ¨0é™„è¿‘æ¥æé«˜æ”¶æ•›é€Ÿåº¦ã€‚
  $$
  x_{i}:=\frac{x_{i}-\mu_{i}}{s_{i}}
  $$
  Where $Î¼_i$ is the **average** of all the values for feature $(i)$ and $s_i$ is the range of values $(max - min)$, or $s_i$ is the standard deviation.

  æ²¡æœ‰æ§åˆ¶å‚æ•°èŒƒå›´å°±ä¼šå˜æˆè¿™æ ·ï¼š

  <img src="https://raw.githubusercontent.com/mm0806son/Images/main/202109141508887.png" style="zoom:25%;" />

- Choose learning rate $\alpha$

  æ•°å­¦è¯æ˜åªè¦$\alpha$è¶³å¤Ÿå°ï¼Œæ€»ä¼šæ”¶æ•›ã€‚åªä¸è¿‡å¤ªå°äº†æ”¶æ•›å¾ˆæ…¢ã€‚

  

  **Debugging gradient descent.** Make a plot with *number of iterations* on the x-axis. Now plot the cost function, $J(\theta)$ over the number of iterations of gradient descent. å¦‚æœ$J(\theta)$éšç€è¿­ä»£å˜å¤§æˆ–è€…éœ‡è¡ï¼ˆæˆ–è€…æ”¶æ•›å¾ˆæ…¢ï¼‰ï¼Œè¯´æ˜$\alpha$å¤ªå¤§äº†ã€‚

  **Automatic convergence test.** Declare convergence if $J(\theta)$ decreases by less than E in one iteration, where $E$ is some small value such as $10^{âˆ’3}$. However in practice it's difficult to choose this threshold value.

- Polynomial Regression

  ä½¿ç”¨å…¶ä»–å‡½æ•°å½¢å¼ã€‚

  *åæ–‡ä¼šè®²åˆ°ä¸€ç§æ–¹æ³•å¯ä»¥è‡ªåŠ¨æ‰¾æœ€åˆé€‚çš„å‡½æ•°ï¼Ÿ*

### Computing Parameters Analytically (Normal equation method)

å³ç›´æ¥ç®—å‡ºæ¥æœ€å°å€¼æ—¶çš„$\theta$ã€‚

ä¸ç”¨åšFeature Scalingï¼Œä¸ç”¨é€‰$\alpha$ï¼Œä¸ç”¨è¿­ä»£ã€‚ä½†æ˜¯$n$å¤§($\ge10000$)çš„æ—¶å€™ç®—å¾—å¾ˆæ…¢ã€‚

> With the normal equation, computing the inversion has complexity $\mathcal{O}(n^3)$

$$
\theta=\left(X^{T} X\right)^{-1} X^{T} y
$$

> ä¸ºä»€ä¹ˆç”¨è¿™ä¸ªå¼å­ï¼Ÿ-> å»æŸ¥çŸ©é˜µçš„å¾®åˆ†å…¬å¼

é€ æˆ$\left(X^{T} X\right)$ä¸å¯é€†çš„åŸå› ï¼š

- çº¿æ€§ç›¸å…³çš„å‚æ•° (size in feet$^2$ & in m$^2$)
- å‚æ•°é‡è¶…è¿‡äº†æ•°æ®é‡ -> åˆ æ‰ä¸€éƒ¨åˆ†å‚æ•° / use regularization *æ™šç‚¹è®²*

ä½†æ˜¯octaveä½¿ç”¨pseudo intä¹Ÿèƒ½ç®—å‡ºæ­£ç¡®çš„è§£ã€‚

## Week 3

> åˆ†ç±»é—®é¢˜ç”¨çº¿æ€§å›å½’æ˜¯å¾ˆç³Ÿç³•çš„ï¼Œå› ä¸ºå¾ˆé å³çš„ä¸€ä¸ªå€¼ä¼šæŠŠæ•´ä¸ªç›´çº¿å¾€å³æ‹‰ï¼Œä¹Ÿä¼šç»™å‡ºå¤§äº1æˆ–å°äº0çš„é¢„æµ‹å€¼...

### Classification and Representation

Our new form uses the "**Sigmoid Function**", also called the "Logistic Function":
$$
\begin{aligned}
&h_{\theta}(x)=g\left(\theta^{T} x\right) \\
&z=\theta^{T} x \\
&g(z)=\frac{1}{1+e^{-z}}
\end{aligned}
$$
$h_Î¸(x)$ will give us the **probability** that our output is 1.

The **decision boundary** is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.

### Logistic Regression Model

> åŸæ¥çš„æŸå¤±å‡½æ•°ä¸æ˜¯å‡¸å‡½æ•°ï¼Œä¸èƒ½ç”¨æå°å€¼å½“æœ€å°å€¼äº†...

$$
\begin{array}{ll}
J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \operatorname{Cost}\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right) & \\
\operatorname{Cost}\left(h_{\theta}(x), y\right)=-\log \left(h_{\theta}(x)\right) & \text { if } \mathrm{y}=1 \\
\operatorname{Cost}\left(h_{\theta}(x), y\right)=-\log \left(1-h_{\theta}(x)\right) & \text { if } \mathrm{y}=0
\end{array}
$$

æœ‰å¦‚ä¸‹æ€§è´¨ï¼š

$\operatorname{Cost}\left(h_{\theta}(x), y\right)=0$ if $h_{\theta}(x)=y$ 
$\operatorname{Cost}\left(h_{\theta}(x), y\right) \rightarrow \infty$ if $y=0$ and $h_{\theta}(x) \rightarrow 1$
$\operatorname{Cost}\left(h_{\theta}(x), y\right) \rightarrow \infty$ if $y=1$ and $h_{\theta}(x) \rightarrow 0$

å¯ä»¥åŒ–ç®€ä¸ºï¼š
$$
\operatorname{Cost}\left(h_{\theta}(x), y\right)=-y \log \left(h_{\theta}(x)\right)-(1-y) \log \left(1-h_{\theta}(x)\right)
$$

$$
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]
$$

A vectorized implementation is:
$$
\begin{aligned}
&h=g(X \theta) \\
&J(\theta)=\frac{1}{m} \cdot\left(-y^{T} \log (h)-(1-y)^{T} \log (1-h)\right)
\end{aligned}
$$
Gradientï¼š
$$
\begin{aligned}
&\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta)\\
&\theta_{j}:=\theta_{j}-\frac{\alpha}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}\\
&\theta:=\theta-\frac{\alpha}{m} X^{T}(g(X \theta)-\vec{y})
\end{aligned}
$$
å’Œä¹‹å‰çº¿æ€§å›å½’æ–¹æ³•çš„å…¬å¼æ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯è¿™é‡Œçš„$h(x)$å˜äº†ã€‚

è¿˜æœ‰å¾ˆå¤šä¼˜åŒ–æ–¹æ³•ï¼šConjugate gradient, BFGS, L-BFGS.. ä¸éœ€è¦æ‰‹åŠ¨é€‰æ‹©$\alpha$ï¼Œé€šå¸¸æ¯”æ¢¯åº¦ä¸‹é™è¦å¿«ã€‚ä½†æ˜¯æ›´å¤æ‚ã€‚

Octaveé‡Œå·²ç»åŒ…å«äº†ä¸€äº›å‡½æ•°ï¼š

```matlab
options = optimset('GradObj', 'on', 'MaxIter', 100);
initialTheta = zeros(2,1);
   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);

```

```matlab
function [jVal, gradient] = costFunction(theta)
  jVal = [...code to compute J(theta)...];
  gradient = [...code to compute derivative of J(theta)...];
end
```

### Multiclass Classification

æ€è·¯æ˜¯åˆ†åˆ«åŒºåˆ†æ¯ä¸€ä¸ªç±»å‹ï¼š

<img src="https://raw.githubusercontent.com/mm0806son/Images/main/202109231136281.png" style="zoom: 25%;" />

### Solving the Problem of Overfitting

Underfit -> High bias

Overfit -> High **variance**

There are two main options to address the issue of overfitting:

1) Reduce the number of features:

- Manually select which features to keep.
- Use a model selection algorithm (studied later in the course).

2) Regularization

- Keep all the features, but reduce the magnitude of parameters $\theta_j$.
- Regularization works well when we have a lot of slightly useful features.

è¶Šç®€å•çš„Hypothesisè¶Šä¸å®¹æ˜“Overfitã€‚

#### Regularization in Linear Regression

æˆ‘ä»¬åœ¨ç½šå‡½æ•°ä¸­æ·»åŠ ç¬¬äºŒé¡¹å¯¹ç³»æ•°$\theta$è¿›è¡Œæ§åˆ¶ï¼ˆé™¤äº†$\theta_0$ä»¥å¤–ï¼‰ï¼š 
$$
\min _{\theta} \frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}
$$
å…¬å¼å˜ä¸ºï¼š
$$
\begin{aligned}
&\theta_{0}:=\theta_{0}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{0}^{(i)} \\
&\theta_{j}:=\theta_{j}-\alpha\left[\left(\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}\right)+\frac{\lambda}{m} \theta_{j}\right] \quad j \in\{1,2 \ldots n\}\\

\end{aligned}
$$
å³ï¼š
$$
\theta_{j}:=\theta_{j}\left(1-\alpha \frac{\lambda}{m}\right)-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
$$
$1âˆ’Î±\frac{Î»}{m}$æ€»æ˜¯æ¯”1å°ä¸€äº›ï¼Œå› æ­¤æ¯æ¬¡è¿­ä»£éƒ½ä¼šæŠŠ$\theta_j$ç¼©å°ä¸€ç‚¹ã€‚ç¬¬äºŒé¡¹åˆ™å’Œregularizationä¹‹å‰çš„å½¢å¼ä¸€æ ·ã€‚

#### **Normal Equation**

> åŒæ ·å¯ä»¥ç›´æ¥ç®—å‡ºæ¥

$$
\begin{aligned}
&\theta=\left(X^{T} X+\lambda \cdot L\right)^{-1} X^{T} y\\
&where\ L=\left[\begin{array}{lllll}0 & & & & \\ & 1 & & & \\ & & 1 & & \\ & & & \ddots & \\ & & & & 1\end{array}\right]
\end{aligned}
$$
$X^{T} X+\lambda \cdot L$ æ°¸è¿œå¯é€†ï¼Œè§£å†³äº†Week2æåˆ°çš„å¯èƒ½ä¸å¯é€†çš„é—®é¢˜ã€‚


$$
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}
$$
The second sum, $\sum_{j=1}^n \theta_j^2$ **means to explicitly exclude** the bias term, $\theta_0$. I.e. the $\theta$ vector is indexed from 0 to n (holding n+1 values, $\theta_0$ through $\theta_n$). Thus, when computing the equation, we should continuously update the two following equations:

![](https://raw.githubusercontent.com/mm0806son/Images/main/202109231603063.png)

## Week 4

> å¦‚æœç”¨ä¸Šé¢çš„æ–¹æ³•åˆ—ä¸¾æ‰€æœ‰é«˜æ¬¡é¡¹ç»„åˆæ¥å®ç°éçº¿æ€§ï¼Œå¤æ‚åº¦ä¼šéšç€å‚æ•°ä¸Šå‡è€Œè¿…é€Ÿä¸Šå‡...

### Neural Networks

If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $Î˜^{(j)}$ will be of dimension $s_{j+1}Ã—(s_j+1)$.
$$
\begin{array}{r}
a_{1}^{(2)}=g\left(\Theta_{10}^{(1)} x_{0}+\Theta_{11}^{(1)} x_{1}+\Theta_{12}^{(1)} x_{2}+\Theta_{13}^{(1)} x_{3}\right) \\
a_{2}^{(2)}=g\left(\Theta_{20}^{(1)} x_{0}+\Theta_{21}^{(1)} x_{1}+\Theta_{22}^{(1)} x_{2}+\Theta_{23}^{(1)} x_{3}\right) \\
a_{3}^{(2)}=g\left(\Theta_{30}^{(1)} x_{0}+\Theta_{31}^{(1)} x_{1}+\Theta_{32}^{(1)} x_{2}+\Theta_{33}^{(1)} x_{3}\right) \\
h_{\Theta}(x)=a_{1}^{(3)}=g\left(\Theta_{10}^{(2)} a_{0}^{(2)}+\Theta_{11}^{(2)} a_{1}^{(2)}+\Theta_{12}^{(2)} a_{2}^{(2)}+\Theta_{13}^{(2)} a_{3}^{(2)}\right)
\end{array}
$$
æ³¨æ„è¦åŠ åç§»é‡$\theta_0$ã€‚ä¸Šæ ‡æ‹¬å·é‡Œä»£è¡¨çš„æ˜¯å±‚æ•°ã€‚

<img src="https://raw.githubusercontent.com/mm0806son/Images/main/202109271702585.png" alt="image-20210927170257865" style="zoom: 25%;" />



å…·ä½“çš„æ“ä½œæ˜¯è¿™æ ·çš„ï¼š
$$
z^{(j)}=\Theta^{(j-1)} a^{(j-1)}
$$

$$
a^{(j)}=g\left(z^{(j)}\right)
$$

ç›´åˆ°æœ€åä¸€æ­¥ï¼š
$$
h_{\Theta}(x)=a^{(j+1)}=g\left(z^{(j+1)}\right)
$$
å’Œ logistic regression åšçš„æ˜¯ä¸€æ ·çš„ã€‚

> ç¥ç»ç½‘ç»œå¯ä»¥æ¨¡æ‹Ÿé€»è¾‘ç”µè·¯ï¼Œä¹Ÿå°±æ˜¯è¯´å¯ä»¥ç”¨æœºå™¨ç®—å‡ºé€»è¾‘ç”µè·¯ã€‚

![img](https://raw.githubusercontent.com/mm0806son/Images/main/202109272110134.png)

## Week 5

### **Back propagation Algorithm**

#### Cost Function

> Neural Networkçš„Cost functionæ˜¯Logistic regressionçš„å»¶ä¼¸ã€‚

For Logistic regression: 
$$
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}
$$
For Neural Network:
$$
J(\Theta)=-\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[y_{k}^{(i)} \log \left(\left(h_{\Theta}\left(x^{(i)}\right)\right)_{k}\right)+\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\Theta}\left(x^{(i)}\right)\right)_{k}\right)\right]+\frac{\lambda}{2 m} \sum_{l=1}^{L-1} \sum_{i=1}^{s l} \sum_{j=1}^{s l+1}\left(\Theta_{j, i}^{(l)}\right)^{2}
$$

$K$æ˜¯å±‚æ•°ã€‚

ç¬¬ä¸€éƒ¨åˆ†çš„æ±‚å’Œæ˜¯å¯¹Output Layerçš„æ¯ä¸€å±‚å•ç‹¬ç®—ç„¶åæ±‚å’Œã€‚å…·ä½“æ“ä½œæ˜¯æŠŠçŸ©é˜µæ¯ä¸€é¡¹ä¹˜èµ·æ¥æ±‚å’Œã€‚

```matlab
regularized = lambda/(2*m) * (sum(sum(Theta1(:,2:end).^2)) + sum(sum(Theta2(:,2:end).^2)) ); 
J = 1 / m * sum( sum( -class_y.* log(h) -  (1-class_y).*log(1-h) ))+ regularized;
```

The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.

ä¹‹å‰æˆ‘ä»¬åœ¨è®¡ç®—ç¥ç»ç½‘ç»œé¢„æµ‹ç»“æœçš„æ—¶å€™æˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§æ­£å‘ä¼ æ’­æ–¹æ³•ï¼Œæˆ‘ä»¬ä»ç¬¬ä¸€å±‚å¼€å§‹æ­£å‘ä¸€å±‚ä¸€å±‚è¿›è¡Œè®¡ç®—ï¼Œç›´åˆ°æœ€åä¸€å±‚çš„$â„\theta(x)$ã€‚

æˆ‘ä»¬çš„ç›®æ ‡ä»ç„¶æ˜¯æ‰¾åˆ°$\min _{\Theta} J(\Theta)$ã€‚ç°åœ¨ï¼Œä¸ºäº†è®¡ç®—ä»£ä»·å‡½æ•°çš„åå¯¼æ•°$\frac{\partial}{\partial \Theta_{i, j}^{(l)}} J(\Theta)$ï¼Œæˆ‘ä»¬éœ€è¦é‡‡ç”¨ä¸€ç§**åå‘ä¼ æ’­ç®—æ³•**ï¼Œä¹Ÿå°±æ˜¯é¦–å…ˆè®¡ç®—æœ€åä¸€å±‚çš„è¯¯å·®ï¼Œç„¶åå†ä¸€å±‚ä¸€å±‚åå‘æ±‚å‡ºå„å±‚çš„è¯¯å·®ï¼Œç›´åˆ°å€’æ•°ç¬¬äºŒå±‚ã€‚

#### Neural net gradient function

**Back Propagation å…·ä½“çš„æ“ä½œæ–¹æ³•ï¼š**

Given training set $\left\{\left(x^{(1)}, y^{(1)}\right) \cdots\left(x^{(m)}, y^{(m)}\right)\right\}$
- Set $\Delta_{i, j}^{(l)}:=0$ for all $(1, i, j)$, åˆå§‹åŒ–çŸ©é˜µ

For training example $\mathrm{t}=1$ to $\mathrm{m}$ :

1. $\operatorname{Set} a^{(1)}:=x^{(t)}$

2. Perform forward propagation to compute $a^{(l)}$ for $l=2,3, \ldots, \mathrm{L}$ 
   å…ˆæ­£å‘ç®—å‡ºç»“æœ

   ![](https://raw.githubusercontent.com/mm0806son/Images/main/202110051630507.png)

3. Using $y^{(t)}$, compute $\delta^{(L)}=a^{(L)}-y^{(t)}$ ç®—æœ€åä¸€å±‚çš„è¯¯å·®
   Where $L$ is our total number of layers and $a^{(L)}$ is the vector of outputs of the activation units for the last layer. So our "error values" for the last layer are simply the differences of our actual results in the last layer and the correct outputs in $\mathrm{y}$. 

4. Compute $\delta^{(L-1)}, \delta^{(L-2)}, \ldots, \delta^{(2)}$ using $\delta^{(l)}=\left(\left(\Theta^{(l)}\right)^{T} \delta^{(l+1)}\right) . * a^{(l)} . *\left(1-a^{(l)}\right)$
   The delta values of layer $l$ are calculated by multiplying the delta values in the next layer with the theta matrix of layer $l$. We then element-wise multiply that with a function called $\mathrm{g}^{\prime}$, which is the derivative of the activation function g evaluated with the input values given by $z^{(l)}$.
   The g-prime derivative terms can also be written out as:
   $$
   g^{\prime}\left(z^{(l)}\right)=a^{(l)} \cdot *\left(1-a^{(l)}\right)
   $$

5. $\Delta_{i, j}^{(l)}:=\Delta_{i, j}^{(l)}+a_{j}^{(l)} \delta_{i}^{(l+1)}$ or with vectorization, $\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}\left(a^{(l)}\right)^{T}$
   Hence we update our new $\Delta$ matrix.

   - $D_{i, j}^{(l)}:=\frac{1}{m}\left(\Delta_{i, j}^{(l)}+\lambda \Theta_{i, j}^{(l)}\right)$, if $j \neq 0 .$
   - $D_{i, j}^{(l)}:=\frac{1}{m} \Delta_{i, j}^{(l)}$ , if $j=0$

$ğ‘™$ ä»£è¡¨ç›®å‰æ‰€è®¡ç®—çš„æ˜¯ç¬¬å‡ å±‚ã€‚
$ğ‘—$ ä»£è¡¨ç›®å‰è®¡ç®—å±‚ä¸­çš„æ¿€æ´»å•å…ƒçš„ä¸‹æ ‡ï¼Œä¹Ÿå°†æ˜¯ä¸‹ä¸€å±‚çš„ç¬¬$ğ‘—$ä¸ªè¾“å…¥å˜é‡çš„ä¸‹æ ‡ã€‚
$ğ‘–$ ä»£è¡¨ä¸‹ä¸€å±‚ä¸­è¯¯å·®å•å…ƒçš„ä¸‹æ ‡ï¼Œæ˜¯å—åˆ°æƒé‡çŸ©é˜µä¸­ç¬¬$ğ‘–$è¡Œå½±å“çš„ä¸‹ä¸€å±‚ä¸­çš„è¯¯å·®å•å…ƒ
çš„ä¸‹æ ‡ã€‚



FPæ˜¯æ­£å‘æ¨å¯¼ï¼Œåˆ©ç”¨ä¸Šä¸€æ­¥çš„ç»“æœæ¨åé¢çš„æ•°å€¼ï¼Œç›´åˆ°æœ€åå¾—åˆ°Outputã€‚
BPæ˜¯é€†å‘æ¨å¯¼ï¼Œåˆ©ç”¨åä¸€æ­¥çš„è¯¯å·®æ¨å‰é¢çš„è¯¯å·®ï¼Œç›´åˆ°Layer 2å¾—åˆ°Cost Functionï¼ˆLayer 1æ˜¯åŸå§‹æ•°æ®ï¼Œä¸éœ€è¦è®¡ç®—ï¼‰ã€‚

> å…¶å®ä¹Ÿå¯ä»¥ä»å‰å¾€åä½¿ç”¨Automatic Differentiationï¼Œä½†æ˜¯è¾“å‡ºå±‚åªæœ‰ä¸€ä¸ªå•å…ƒï¼Œæ‰€ä»¥ä½¿ç”¨BPã€‚

æŠŠ$y$å…ˆå±•å¼€åˆ°çŸ©é˜µ`class_y`å½¢å¼ï¼Œæ˜¯ä¸ºäº†è®©é¢„æµ‹é”™è¯¯æ—¶çš„Lossä¸€æ ·ã€‚

### Unrolling parameters

> æŠŠçŸ©é˜µå±•å¼€è¿˜åŸæˆå‘é‡ç”¨äºè®¡ç®—çš„æ–¹æ³•

`thetaVec` æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„åˆ—å‘é‡ï¼ŒæŒ‰é¡ºåºæŠŠçŸ©é˜µçš„æ‰€æœ‰å…ƒç´ æ”¾è¿›å»ã€‚
`reshape` æ˜¯æŠŠå®ƒè¿˜åŸæˆçŸ©é˜µã€‚

```matlab
thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ]
deltaVector = [ D1(:); D2(:); D3(:) ]

Theta1 = reshape(thetaVector(1:110),10,11)
Theta2 = reshape(thetaVector(111:220),10,11)
Theta3 = reshape(thetaVector(221:231),1,11)
```

### Gradient Check

Two-side derivation:
$$
\frac{\partial}{\partial \Theta} J(\Theta) \approx \frac{J(\Theta+\epsilon)-J(\Theta-\epsilon)}{2 \epsilon}
$$
With multiple theta matrices, we can approximate the derivative **with respect to $\Theta_{j}$** as follows:
$$
\frac{\partial}{\partial \Theta_{j}} J(\Theta) \approx \frac{J\left(\Theta_{1}, \ldots, \Theta_{j}+\epsilon, \ldots, \Theta_{n}\right)-J\left(\Theta_{1}, \ldots, \Theta_{j}-\epsilon, \ldots, \Theta_{n}\right)}{2 \epsilon}
$$
ä»£ç å®ç°ï¼š

```matlab
epsilon = 1e-4;
for i = 1:n,
  thetaPlus = theta;
  thetaPlus(i) += epsilon;
  thetaMinus = theta;
  thetaMinus(i) -= epsilon;
  gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)
end;
```

æ£€æŸ¥ç®—å‡ºæ¥çš„å¯¼æ•°å’ŒBPçš„åŸºæœ¬ä¸€è‡´åï¼Œè¦disable gradient checkï¼Œå› ä¸ºç®—çš„å¾ˆæ…¢ã€‚

### Random Initialization

å°†æ‰€æœ‰$\theta$æƒé‡åˆå§‹åŒ–ä¸ºé›¶å¯¹ç¥ç»ç½‘ç»œä¸èµ·ä½œç”¨ã€‚å½“æˆ‘ä»¬BPæ—¶ï¼Œæ‰€æœ‰èŠ‚ç‚¹éƒ½ä¼šé‡å¤æ›´æ–°åˆ°ç›¸åŒçš„å€¼ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä»¥ä¸‹æ–¹æ³•éšæœºåœ°åˆå§‹åŒ–æˆ‘ä»¬çš„$\Theta$çŸ©é˜µçš„æƒé‡ã€‚

![img](https://raw.githubusercontent.com/mm0806son/Images/main/202110052205184.png)

Hence, we initialize each $\Theta^{(l)}_{ij}$ to a random value between $[-\epsilon,\epsilon]$

ä»£ç å®ç°ï¼š

```matlab
If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.

Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON;
```

### **Training a Neural Network**

ä¸€èˆ¬æ¥è¯´hidden layerè¦ä¹ˆåªæœ‰ä¸€å±‚ï¼Œè¦ä¹ˆæ¯å±‚æ•°ç›®éƒ½ç›¸ç­‰ã€‚

å…·ä½“æµç¨‹ï¼š

1. Randomly initialize the weights
2. Implement forward propagation to get $h_\Theta(x^{(i)})$ for any $x^{(i)}$
3. Implement the cost function
4. Implement backpropagation to compute partial derivatives
5. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.
6. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.

> $J(\Theta)$ ä¸æ˜¯å‡¸å‡½æ•°ï¼Œä¸ä¸€å®šèƒ½æ‰¾åˆ°å…¨å±€æœ€å°å€¼äº†ã€‚

## Week 6

### Debugging

#### Evaluating a Hypothesis (Over & Underfitting)

éšæœºåˆ†ä¸€éƒ¨åˆ†æ•°æ®(30%)ä½œä¸ºTest set

For linear regression:
$$
J_{\text {test }}(\Theta)=\frac{1}{2 m_{\text {test }}} \sum_{i=1}^{m_{\text {test }}}\left(h_{\Theta}\left(x_{\text {test }}^{(i)}\right)-y_{\text {test }}^{(i)}\right)^{2}
$$
For classification ~ Misclassification error ( 0/1 misclassification error):
$$
\operatorname{err}\left(h_{\Theta}(x), y\right)=\begin{array}{cc}
1 & \text { if } h_{\Theta}(x) \geq 0.5 \text { and } y=0 \text { or } h_{\Theta}(x)<0.5 \text { and } y=1 \\
0 & \text { otherwise }
\end{array}
$$

$$
\text { Test Error }=\frac{1}{m_{\text {test }}} \sum_{i=1}^{m_{t e s t}} \operatorname{err}\left(h_{\Theta}\left(x_{\text {test }}^{(i)}\right), y_{\text {test }}^{(i)}\right)
$$

#### Model Selection

20% Cross Validation set + 20% Test Set

We can now calculate three separate error values for the three different sets using the following method:

1. Optimize the parameters in Î˜ using the training set for each polynomial degree.
2. Find the polynomial degree d with the least error using the cross validation set.
3. Estimate the generalization error using the test set with $J_{test}(\Theta^{(d)})$, (d = theta from polynomial with lower error);

ç”¨ä¸€éƒ¨åˆ†è®­ç»ƒï¼Œä¸€éƒ¨åˆ†ç”¨æ¥æ‰¾æœ€å¥½çš„ç®—æ³•ï¼Œå†ç”¨ä¸€éƒ¨åˆ†å»ä¼°è®¡é¢„æµ‹æœªçŸ¥æ•°æ®çš„å‡†ç¡®æ€§ã€‚ä¸‰éƒ¨åˆ†äº’ä¸é‡å¤ï¼Œé¿å…ä½¿ç”¨å·²ç»è§è¿‡çš„æ•°æ®å½±å“ç»“æœã€‚è®­ç»ƒé›†ç”¨äºè®­ç»ƒä¸åŒçš„æ¨¡å‹ï¼ŒéªŒè¯é›†ç”¨äºæ¨¡å‹é€‰æ‹©ã€‚è€Œæµ‹è¯•é›†ç”±äºåœ¨è®­ç»ƒæ¨¡å‹å’Œæ¨¡å‹é€‰æ‹©è¿™ä¸¤æ­¥éƒ½æ²¡æœ‰ç”¨åˆ°ï¼Œå¯¹äºæ¨¡å‹æ¥è¯´æ˜¯æœªçŸ¥æ•°æ®ï¼Œå› æ­¤å¯ä»¥ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

#### Bias & Variance

Bias = Underfitting -> $J_{train}$ é«˜ & $J_{cv}$ é«˜ï¼Œ $J_{CV}(\Theta) \approx J_{train}(\Theta)$
Variance = Overfitting ->  $J_{train} \gg  J_{cv}$ 

<img src="https://raw.githubusercontent.com/mm0806son/Images/main/202110122144499.png" alt="image-20211012214442374" style="zoom: 33%;" />

å½“ $\lambda$ è¾ƒå°æ—¶ï¼Œè®­ç»ƒé›†è¯¯å·®è¾ƒå°ï¼ˆè¿‡æ‹Ÿåˆï¼‰è€Œäº¤å‰éªŒè¯é›†è¯¯å·®è¾ƒå¤§ï¼›éšç€ $\lambda$ çš„å¢åŠ ï¼Œè®­ç»ƒé›†è¯¯å·®ä¸æ–­å¢åŠ ï¼ˆæ¬ æ‹Ÿåˆï¼‰ï¼Œè€Œäº¤å‰éªŒè¯é›†è¯¯å·®åˆ™æ˜¯å…ˆå‡å°åå¢åŠ ã€‚

<img src="https://raw.githubusercontent.com/mm0806son/Images/main/202110122227323.png" alt="image-20211012222702202" style="zoom:75%;" />

#### Learning Curves

##### Experiencing high bias

- **Low training set size**: causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high.

- **Large training set size**: causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ to be high with $J_{train}(\Theta)\approx J_{CV}(\Theta)$.

If a learning algorithm is suffering from **high bias**, getting more training data will not **(by itself)** help much.

![](https://raw.githubusercontent.com/mm0806son/Images/main/202110122241170.png)

##### Experiencing high variance

- **Low training set size**: $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high. (**Same**)

- **Large training set size**: $J_{train}(\Theta)$ increases with training set size and $J_{CV}(\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\Theta) \lt J_{CV}(\Theta)$ but the difference between them remains significant.

If a learning algorithm is suffering from **high variance**, getting more training data is likely to help.

![](https://raw.githubusercontent.com/mm0806son/Images/main/202110122247590.png)

#### System Design Example

> ä¸€ä¸ªåƒåœ¾é‚®ä»¶åˆ†æ‹£å™¨...

æˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸€ä¸ªç”±100 ä¸ªæœ€å¸¸å‡ºç°åœ¨åƒåœ¾é‚®ä»¶ä¸­çš„è¯æ‰€æ„æˆçš„åˆ—è¡¨ï¼Œæ ¹æ®è¿™äº›è¯æ˜¯å¦æœ‰åœ¨é‚®ä»¶ä¸­å‡ºç°ï¼Œæ¥è·å¾—æˆ‘ä»¬çš„ç‰¹å¾å‘é‡ï¼ˆå‡ºç°ä¸º1ï¼Œä¸å‡ºç°ä¸º0ï¼‰ã€‚

ä¸ºäº†æ„å»ºè¿™ä¸ªåˆ†ç±»å™¨ç®—æ³•ï¼Œæˆ‘ä»¬å¯ä»¥åšå¾ˆå¤šäº‹ï¼Œä¾‹å¦‚ï¼š
1. æ”¶é›†æ›´å¤šçš„æ•°æ®ï¼Œè®©æˆ‘ä»¬æœ‰æ›´å¤šçš„åƒåœ¾é‚®ä»¶å’Œéåƒåœ¾é‚®ä»¶çš„æ ·æœ¬ï¼›
2. åŸºäºé‚®ä»¶çš„è·¯ç”±ä¿¡æ¯å¼€å‘ä¸€ç³»åˆ—å¤æ‚çš„ç‰¹å¾ï¼›
3. åŸºäºé‚®ä»¶çš„æ­£æ–‡ä¿¡æ¯å¼€å‘ä¸€ç³»åˆ—å¤æ‚çš„ç‰¹å¾ï¼ŒåŒ…æ‹¬è€ƒè™‘æˆªè¯çš„å¤„ç†ï¼›
4. ä¸ºæ¢æµ‹åˆ»æ„çš„æ‹¼å†™é”™è¯¯ï¼ˆæŠŠwatch å†™æˆw4tchï¼‰å¼€å‘å¤æ‚çš„ç®—æ³•ã€‚

##### Error Analysis

æ„å»ºä¸€ä¸ªå­¦ä¹ ç®—æ³•çš„æ¨èæ–¹æ³•ä¸ºï¼š
1. ä»ä¸€ä¸ªç®€å•çš„èƒ½å¿«é€Ÿå®ç°çš„ç®—æ³•å¼€å§‹ï¼Œå®ç°è¯¥ç®—æ³•å¹¶ç”¨äº¤å‰éªŒè¯é›†æ•°æ®æµ‹è¯•è¿™ä¸ªç®—æ³•ï¼›
2. ç»˜åˆ¶å­¦ä¹ æ›²çº¿ï¼Œå†³å®šæ˜¯å¢åŠ æ›´å¤šæ•°æ®ï¼Œæˆ–è€…æ·»åŠ æ›´å¤šç‰¹å¾ï¼Œè¿˜æ˜¯å…¶ä»–é€‰æ‹©ï¼›
3. è¿›è¡Œè¯¯å·®åˆ†æï¼šäººå·¥æ£€æŸ¥äº¤å‰éªŒè¯é›†ä¸­æˆ‘ä»¬ç®—æ³•ä¸­äº§ç”Ÿé¢„æµ‹è¯¯å·®çš„å®ä¾‹ï¼Œçœ‹çœ‹è¿™äº›å®ä¾‹æ˜¯å¦æœ‰æŸç§ç³»ç»ŸåŒ–çš„è¶‹åŠ¿ã€‚

ä»¥æˆ‘ä»¬çš„åƒåœ¾é‚®ä»¶è¿‡æ»¤å™¨ä¸ºä¾‹ï¼Œè¯¯å·®åˆ†æè¦åšçš„æ—¢æ˜¯æ£€éªŒäº¤å‰éªŒè¯é›†ä¸­æˆ‘ä»¬çš„ç®—æ³•äº§ç”Ÿé”™è¯¯é¢„æµ‹çš„æ‰€æœ‰é‚®ä»¶ï¼Œçœ‹ï¼šæ˜¯å¦èƒ½å°†è¿™äº›é‚®ä»¶æŒ‰ç…§ç±»åˆ†ç»„ã€‚ä¾‹å¦‚åŒ»è¯å“åƒåœ¾é‚®ä»¶ï¼Œä»¿å†’å“åƒåœ¾é‚®ä»¶æˆ–è€…å¯†ç çªƒå–é‚®ä»¶ç­‰ã€‚ç„¶åçœ‹åˆ†ç±»å™¨å¯¹å“ªä¸€ç»„é‚®ä»¶çš„é¢„æµ‹è¯¯å·®æœ€å¤§ï¼Œå¹¶ç€æ‰‹ä¼˜åŒ–ã€‚æ€è€ƒæ€æ ·èƒ½æ”¹è¿›åˆ†ç±»å™¨ã€‚ä¾‹å¦‚ï¼Œå‘ç°æ˜¯å¦ç¼ºå°‘æŸäº›ç‰¹å¾ï¼Œè®°ä¸‹è¿™äº›ç‰¹å¾å‡ºç°çš„æ¬¡æ•°ã€‚ä¾‹å¦‚è®°å½•ä¸‹é”™è¯¯æ‹¼å†™å‡ºç°äº†å¤šå°‘æ¬¡ï¼Œå¼‚å¸¸çš„é‚®ä»¶è·¯ç”±æƒ…å†µå‡ºç°äº†å¤šå°‘æ¬¡ç­‰ç­‰ï¼Œç„¶åä»å‡ºç°æ¬¡æ•°æœ€å¤šçš„æƒ…å†µå¼€å§‹ç€æ‰‹ä¼˜åŒ–ã€‚

> å…ˆå®ç°æœ€åŸºç¡€çš„ç®—æ³•ï¼Œå†æ·»åŠ ä¸œè¥¿çœ‹çœ‹æ˜¯ä¸æ˜¯å˜å¥½äº†...

##### Precision/Recall

> Skewed Classesï¼šåªæœ‰0.5%çš„ç—…äººï¼Œç›´æ¥è¯´éƒ½æ²¡ç—…éƒ½æ¯”1%æ­£ç¡®ç‡çš„ç®—æ³•å¥½â€¦

<img src="https://raw.githubusercontent.com/mm0806son/Images/main/202110142200784.png" style="zoom:33%;" />
$$
\begin{aligned}
&\text {æŸ¥å‡†ç‡ Precision }=\frac{\text { True positives }}{\# \text { predicted as positive }}=\frac{\text { True positives }}{\text { True positives }+\text { False positives }} \\
&\text {æŸ¥å…¨ç‡ Recall }=\frac{\text { True positives }}{\# \text { actual positives }}=\frac{\text { True positives }}{\text { True positives }+\text { False negatives }}
\end{aligned}
$$
æŠŠæ›´ç½•è§çš„æ•°æ®é›†å®šä¸º$y=1$ã€‚

æé«˜åˆ¤æ–­é˜ˆå€¼å¯ä»¥æé«˜æŸ¥å‡†ç‡Precisionï¼Œé™ä½æŸ¥å…¨ç‡Recallã€‚åä¹‹äº¦ç„¶ã€‚

åˆ¤æ–­ç®—æ³•å¥½åçš„ä¾æ®ï¼š
$$
F_{1} \text { Score: } 2 \frac{P R}{P+R}
$$




